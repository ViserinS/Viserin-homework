##  作业

## IT伦理与道德

###  1、网络病毒

####  例子：勒索病毒：

勒索病毒，是一种新型电脑病毒，主要以邮件、程序木马、网页挂马的形式进行传播。该病毒性质恶劣、危害极大，一旦感染将给用户带来无法估量的损失。这种病毒利用各种加密算法对文件进行加密，被感染者一般无法解密，必须拿到解密的私钥才有可能破解。

勒索病毒文件一旦进入本地，就会自动运行，同时删除勒索软件样本，以躲避查杀和分析。接下来，勒索病毒利用本地的互联网访问权限连接至黑客的C&C服务器，进而上传本机信息并下载加密私钥与公钥，利用私钥和公钥对文件进行加密。除了病毒开发者本人，其他人是几乎不可能解密。加密完成后，还会修改壁纸，在桌面等明显位置生成勒索提示文件，指导用户去缴纳赎金。且变种类型非常快，对常规的杀毒软件都具有免疫性。攻击的样本以exe、js、wsf、vbe等类型为主，对常规依靠特征检测的安全产品是一个极大的挑战。 [2] 
据火绒监测，勒索病毒主要通过三种途径传播：漏洞、邮件和广告推广。
通过漏洞发起的攻击占攻击总数的87.7%。由于win7、xp等老旧系统存在大量无法及时修复的漏洞，而政府、企业、学校、医院等局域网机构用户使用较多的恰恰是win7、xp等老旧系统，因此也成为病毒攻击的重灾区，病毒可以通过漏洞在局域网中无限传播。相反，win10系统因为强制更新，几乎不受漏洞攻击的影响。
通过邮件与广告推广的攻击分别为7.4%、3.9%。虽然这两类传播方式占比较少，但对于有收发邮件、网页浏览需求的企业而言，依旧会受到威胁。
此外，对于某些特别依赖U盘、记录仪办公的局域网机构用户来说，外设则成为勒索病毒攻击的特殊途径。

![img](https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=8596ba1cac8b87d6444fa34d6661435d/203fb80e7bec54e7005b1233b4389b504fc26a63.jpg)



####  相关事件

2017年5月12日，一种名为“想哭”的勒索病毒袭击全球150多个国家和地区，影响领域包括政府部门、医疗服务、公共交通、邮政、通信和汽车制造业。
2017年6月27日，欧洲、北美地区多个国家遭到“NotPetya”病毒攻击。乌克兰受害严重，其政府部门、国有企业相继“中招”。
2017年10月24日，俄罗斯、乌克兰等国遭到勒索病毒“坏兔子”攻击。乌克兰敖德萨国际机场、首都基辅的地铁支付系统及俄罗斯三家媒体中招，德国、土耳其等国随后也发现此病毒。 [7] 
2018年2月，多家互联网安全企业截获了Mind Lost勒索病毒。 [8] 
2018年2月，中国内便再次发生多起勒索病毒攻击事件。经腾讯企业安全分析发现，此次出现的勒索病毒正是GlobeImposter家族的变种，该勒索病毒将加密后的文件重命名为.GOTHAM、.Techno、.DOC、.CHAK、.FREEMAN、.TRUE、.TECHNO等扩展名，并通过邮件来告知受害者付款方式，使其获利更加容易方便。 [9] 
2018年3月1日，有杀毒软件厂商表示，他们监测到了“麒麟2.1”的勒索病毒。 [10] 
2018年3月，国家互联网应急中心通过自主监测和样本交换形式共发现23个锁屏勒索类恶意程序变种。该类病毒通过对用户手机锁屏，勒索用户付费解锁，对用户财产和手机安全均造成严重威胁。 [11] 
从2018年初到9月中旬，勒索病毒总计对超过200万台终端发起过攻击，攻击次数高达1700万余次，且整体呈上升趋势。

精通IT知识的专业人员，如果没有道德，很可能为了利益利用自己的知识编辑病毒来达到目的。

### 人工智能伦理道德

1、隐性的机器歧视。现阶段，机器学习算法也好，深度学习算法也罢，都能决定用户的行为，如看什么视频、听什么歌曲；甚至还能决定获得贷款、救助金的主体以及具体金额。为用户推荐好友、根据搜索记录自动为用户推荐商品、对用户的信用进行评估、对应聘者的能力进行评估、对犯罪风险进行评估等。越来越多的人类活动交给了人工智能机器来裁决，我们认为人工智能机器是公平的，但人工智能机器是否公平却是一个未知数，其中存在巨大的公平隐患。

2、不容忽视的安全与隐私。人工智能依赖于算法，但算法却具有不透明性和不可预见性，增加了人工智能监管失控的发生几率，从而产生一系列安全问题。例如，2015年德国大众汽车制造厂发生的机器人袭击工作人员事件；2016年发生的谷歌无人驾驶汽车与大巴车碰撞事件。这些人工智能伤人事件发生之后如何明确责任？如何将人工智能带来的安全隐患降到最低？此外，随着人工智能的发展，收集、利用的数据将越来越多，如何保护数据安全也是一大挑战。

3、军事。人工智能（AI）和机器人领域的专家面临一个重要的伦理决策：必须要决定他们是支持还是反对致命自主武器系统（LAWS）。此类系统能在几年而非几十年内发展为可行的。该领域的风险很高，致命自主武器系统被描述为战争的第三次革命，前两次是火药和核武器。

​    在几年之内，军方就可以组装带有武器的四轴飞行器和微型坦克，这些自动的飞行器和坦克不需要人类干预，自己能决定谁能活下来而谁又会死去。例如，四轴飞行器能在一座城市中找到并打击敌方战士，但不包括人类能设定目标的巡航导弹和遥远的无人驾驶飞机。

​    但是，国际人道法律对于此类技术没有任何具体的规定，现在也还不清楚国际社会是否会支持一个限制或禁止此类武器系统的条约。1949年签订的战争人道主义《日内瓦公约》要求任何袭击需要满足3个条件：军事必需、参与战争者和非参战者差别对待、军事目标价值及其潜在附带伤害的平衡。而当前的自动武器很难或根本无法作出这些主观判断。![img](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1542884110560&di=28908cd2dfdcee60f4c1e274d4534859&imgtype=0&src=http%3A%2F%2Fimg95.699pic.com%2Fphoto%2F50062%2F4783.jpg_wh860.jpg)





当人工智能越来越深入人们的生活，其潜在的危险更应引起我们的注意。